{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a57cae0",
   "metadata": {},
   "source": [
    "# ML Module documentation\n",
    "\n",
    "### About\n",
    "This IPython notebook uses quantum-chemistry outputs of third-party programs to train a Kernel Ridge Regression (KRR) Machine Learning Model. The goal of this Model is to use low-accuracy data (e.g. semi-empirical calculations) to predict higher-accuracy data (e.g. excitation energies in TDDFT quality). The notebook requires that the underlying database has been set up consistently for use with the ArchOnML package. This can be achieved by generating it with the related 01...ipynb notebook. Note, that the ML Module will work irrespective of whether this is a conformational space or chemical space scan.\n",
    "\n",
    "The notebook is divided in two different sections that can run independent from each other. The sections and their purposes are:\n",
    "\n",
    "# \n",
    "### Part A - Training and Saving a ML Model.\n",
    "Part A starts with defining what how data should be prepared for machine learning. This is followed by loading the precomputed Training data and forming descriptors out of the raw data. Note, that at this point also the labels should be available for the training set (i.e. the property you want to learn to predict). The file containing the label data should be placed inside the SemiEmpData folder with the same name as the Label. For more details, please refer to the documentation.\n",
    "After that, the distance matrices between all entries are precalculated and stored by feeding the entries into a DataTens object. This object is capable of splitting the data into training/testing sets of user-defined sizes and to prepare the data in K stratified blocks for K-fold cross-validation of hyperparameters.\n",
    "Finally, the MLModel object is created and trained in a (here 5-fold) cross-validation. Here, each hyperparameter scan is a separate program block to monitor each of the somewhat time-consuming steps carefully. Finally, the model is finalized and stored to hard disk.\n",
    "\n",
    "# \n",
    "### Part B - Loading and Predicting with a ML Model.\n",
    "Part B is an example for how to load a previously trained model and run predictions. Note that this part can be run fully independent from Part A, , allowing to run predictions in parallel batches on a high-performance computing cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "from archonml.entries import MLEntry\n",
    "from archonml.desctools import Descriptor\n",
    "from archonml.krr_tens import DataTens\n",
    "from archonml.krr_model import MLModel\n",
    "from archonml.utils import timeconv\n",
    "\n",
    "# Console Options / Quality of Life improvements\n",
    "plt.rcParams['figure.figsize'] = [4, 3]\n",
    "plt.rcParams['figure.dpi'] = 200 # 200 e.g. is really fine, but slower\n",
    "IPythonConsole.ipython_useSVG=True\n",
    "IPythonConsole.drawOptions.addAtomIndices = True\n",
    "IPythonConsole.molSize = 400, 400\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1576a",
   "metadata": {},
   "source": [
    "# PART A Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ee34a",
   "metadata": {},
   "source": [
    "# Step 1 - Inputs for setting up the ML entries and descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deff4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the folder hierarchy information of the current project from DBSpecs.cfg (generated by 01_DB_Starter.ipynb)\n",
    "MLEntry.get_db_specs()\n",
    "\n",
    "# Specify, what the data to train on looks like.\n",
    "# Note that the MOWin attribute is somewhat special, as it not only defines how many orbitals around the HOMO-LUMO orbitals are to be considered, but it also determines how many will be written to the\n",
    "# native MergeFile when wrtiting them for the first time.\n",
    "LType         = \"Training\"\n",
    "LQC_Pack      = \"orca\"\n",
    "LQC_low       = \"PM3\"\n",
    "LQC_high      = \"TD-DFT\"\n",
    "LLabelL       = [\"\"]\n",
    "LDataL        = [\"Geometry\", \"Mulliken_F\", \"SEmpOrbInfo_F\"]\n",
    "MLEntry.MOWin = 4\n",
    "\n",
    "# Predefine which Descriptors should be used and set up the Descriptor Instance.\n",
    "# Note that each descriptor will require at least N**2 distances to be calculated, with N being the number of training entries.\n",
    "# Further, the amount of training data scales with approximately 2*(N**2)*(MOWin**2) if orbital data is to be used.\n",
    "LDescL    = [\"SEmpOrbEnDiffs\", \"SEmpOccs\", \"SEmpVirs\", \"SEmpTups\", \"SEmpEigCoul\", \"SEmpOccEigCoul\",\n",
    "             \"SEmpNEl\", \"SEmpOccPCMEigCoul\", \"SEmpVirPCMEigCoul\", \"SEmpTransPCMEigCoul\", \"SEmpOccVirPTransSum\", \"SEmpHOLUPDiff\"]\n",
    "DescInst  = Descriptor(LDescL)\n",
    "MLOBJLIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b6ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Training IDs and Fingerprints from a Calculation Library. Initialize the MLEntry objects.\n",
    "# If you have several calculation libraries, just copy this cell and run it with another file opened as FID.\n",
    "FID = open(\"./SampleCalcs_1_PROJ\", \"r\")\n",
    "FLoc = FID.readlines()\n",
    "FID.close()\n",
    "LID      = []\n",
    "LFingerP = []\n",
    "for line in range(len(FLoc)):\n",
    "    LID.append(FLoc[line].split()[0])\n",
    "    Aux     = FLoc[line].split()[1]\n",
    "    FingLen = len(Aux.split(\",\"))\n",
    "    LFing = []\n",
    "    for jj in range(FingLen):\n",
    "        LFing.append(int(Aux.split(\",\")[jj]))\n",
    "    LFingerP.append(LFing)\n",
    "    LObj = MLEntry(LID[-1], LType, LFingerP[-1], LQC_Pack, LQC_low, LQC_high, LDataL, LLabelL)\n",
    "    MLOBJLIST.append(dc(LObj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed once per \"newly calculated entry\": Merge raw data into the native format of ArchOnML.\n",
    "cnt   = 0\n",
    "tTot  = 0\n",
    "incnt = 0\n",
    "mReq  = 0\n",
    "now   = time.time()\n",
    "for ii in range(len(MLOBJLIST)):\n",
    "    MLOBJLIST[ii].merge_data()\n",
    "    cnt  += 1\n",
    "    Perc  = cnt / len(MLOBJLIST)\n",
    "    UpdThrsh = 0.001\n",
    "    if Perc > UpdThrsh+(incnt*UpdThrsh):\n",
    "        then  = time.time()\n",
    "        tReq  = then-now\n",
    "        tTot += tReq\n",
    "        mReq  = tTot / cnt\n",
    "        Rem   = float(len(MLOBJLIST)-cnt)*mReq\n",
    "        clear_output(wait=True)\n",
    "        STR1  = \"Finished {:.2f} % ({}) of all structures ({}) in {:.1f} seconds. ({})\\n\".format(Perc*100, cnt, len(MLOBJLIST), tTot, timeconv(tTot),)\n",
    "        STR2  = \"Required {:.3f} seconds on average for each structure.\\n\".format(mReq)\n",
    "        STR3  = \"Expecting {:.1f} seconds remaining. ({})\\n\".format(Rem, timeconv(Rem))\n",
    "        print(STR1+STR2+STR3)\n",
    "        incnt += 1\n",
    "        now    = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800a0c5",
   "metadata": {},
   "source": [
    "# Step 2 - Initialise Descriptors for every MLEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6036cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MLEntry.Data from MergeFiles and Describe immediately.\n",
    "# This step can be commented out, if a \"described MLOBJLIST\" has been saved to a pickle file already (see below). \n",
    "cnt   = 0\n",
    "tTot  = 0\n",
    "incnt = 0\n",
    "now   = time.time()\n",
    "for ii in range(len(MLOBJLIST)):\n",
    "    MLOBJLIST[ii].get_merged_data()\n",
    "    DescInst.describe(MLOBJLIST[ii])\n",
    "    CurID = MLOBJLIST[ii].ID\n",
    "    print(CurID)\n",
    "    cnt  += 1\n",
    "    Perc  = cnt / len(MLOBJLIST)\n",
    "    UpdThrsh = 0.001\n",
    "    if Perc > UpdThrsh+(incnt*UpdThrsh):\n",
    "        then  = time.time()\n",
    "        tReq  = then-now\n",
    "        tTot += tReq\n",
    "        mReq  = tTot / cnt\n",
    "        Rem   = float(len(MLOBJLIST)-cnt)*mReq\n",
    "        clear_output(wait=True)\n",
    "        STR1  = \"Finished {:.2f} % ({}) of all structures ({}) in {:.1f} seconds. ({})\\n\".format(Perc*100, cnt, len(MLOBJLIST), tTot, timeconv(tTot),)\n",
    "        STR2  = \"Required {:.3f} seconds on average for each structure.\\n\".format(mReq)\n",
    "        STR3  = \"Expecting {:.1f} seconds remaining. ({})\\n\".format(Rem, timeconv(Rem))\n",
    "        print(STR1+STR2+STR3)\n",
    "        incnt += 1\n",
    "        now    = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23928829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a \"described MLOBJLIST\" to a pickle file. Only needed when first saving the file, or if the set changes.\n",
    "# Can be commented out afterwards.\n",
    "\n",
    "import pickle\n",
    "PICKLE = \"LearnSet5k.pkl\"\n",
    "with open(PICKLE, \"wb\") as OID:\n",
    "    for ii in range(len(MLOBJLIST)):\n",
    "        pickle.dump(MLOBJLIST[ii], OID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a \"described MLOBJLIST\" from a pickle file. Should be uncommented, if a file exists.\n",
    "\n",
    "# import pickle\n",
    "# PICKLE = \"LearnSet5k.pkl\"\n",
    "# AUX = []\n",
    "# with open(PICKLE, \"rb\") as FID:\n",
    "#     for ii in range(len(MLOBJLIST)):\n",
    "#         AUX.append(pickle.load(FID))\n",
    "# MLOBJLIST = dc(AUX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12821b6f",
   "metadata": {},
   "source": [
    "# Step 3 - Turning Descriptors into KRR tensor format via DataTens instance.\n",
    "Includes Setting up the stratified Cross-Validation IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbcc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataTens Object. Below are the default options for data management.\n",
    "DataTens.CVGridPts    = 64\n",
    "DataTens.CVSize       = 5\n",
    "DataTens.RandomStrat  = True\n",
    "DataTens.FixedSeed    = True\n",
    "DataTens.MinMod       =  0.01   # These have been changed for the conformer scan.\n",
    "DataTens.MaxMod       =  1000   # These have been changed for the conformer scan.\n",
    "DataTens.Lambda_Bot   =  -10\n",
    "DataTens.Lambda_Top   =  -1\n",
    "DataTens.BinWinThrsh  = 10\n",
    "DataTens.BinWinEscape = 100\n",
    "\n",
    "# At initialization, it will precalculate all distance matrices.\n",
    "DataInst = DataTens(MLOBJLIST, mem_mode=\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a196b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into Training and Test set.\n",
    "# Given Percentage is the percentage of the training set.\n",
    "# Whether an entry is considered Test or Training is written to DataInst.Status\n",
    "DataInst.train_test_split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random subportion Active / Inactive. By changing this percentage you obtain the different points on a learning curve.\n",
    "DataInst.train_test_split(_, UPDATE=True, ActPerc=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Stratify the Data with respect to the desired Label.\n",
    "# This leads to the formation of the CVIDBlocks.\n",
    "DataInst.stratify(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the stratified entries.\n",
    "# Checking the distributions:\n",
    "LabelBlocks = []\n",
    "for ii in range(DataTens.CVSize):\n",
    "    LocList = DataInst.CVIDBlocks[ii]\n",
    "    Aux = []\n",
    "    for jj in range(len(LocList)):\n",
    "        Aux.append(DataInst.Labels[\"\"][LocList[jj]])\n",
    "    LabelBlocks.append(Aux)\n",
    "counts, bins, bars = plt.hist(LabelBlocks, bins=15) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3dd019",
   "metadata": {},
   "source": [
    "# Step 4 - Cross-Validation Training for a ML-Model w.r.t. a desired Label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Model with respect to one of the Labels.\n",
    "KRR = MLModel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c22e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CV Tensor for training against Hyperparameters.\n",
    "# Here, Block \"0\" for validation.\n",
    "CV_VaIDs = DataInst.CVIDBlocks[0]\n",
    "CV_TrIDs = DataInst.CVIDBlocks[1] + DataInst.CVIDBlocks[2] + DataInst.CVIDBlocks[3] + DataInst.CVIDBlocks[4]\n",
    "DataInst.update_cv_tens(CV_TrIDs, CV_VaIDs, MLOBJLIST)\n",
    "\n",
    "# Determine the best Hyperparameters SIGMA and LAMBDA for this CV step\n",
    "KRR.cv_para(DataInst, MLOBJLIST, optimizer=\"vectorized\", convR=0.001, convM=0.00001, MaxIter=4, ignoreConv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CV Tensor for training against Hyperparameters.\n",
    "# Here, Block \"1\" for validation.\n",
    "CV_VaIDs = DataInst.CVIDBlocks[1]\n",
    "CV_TrIDs = DataInst.CVIDBlocks[0] + DataInst.CVIDBlocks[2] + DataInst.CVIDBlocks[3] + DataInst.CVIDBlocks[4]\n",
    "DataInst.update_cv_tens(CV_TrIDs, CV_VaIDs, MLOBJLIST)\n",
    "\n",
    "# Determine the best Hyperparameters SIGMA and LAMBDA for this CV step\n",
    "KRR.cv_para(DataInst, MLOBJLIST, optimizer=\"vectorized\", convR=0.001, convM=0.00001, MaxIter=4, ignoreConv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CV Tensor for training against Hyperparameters.\n",
    "# Here, Block \"2\" for validation.\n",
    "CV_VaIDs = DataInst.CVIDBlocks[2]\n",
    "CV_TrIDs = DataInst.CVIDBlocks[0] + DataInst.CVIDBlocks[1] + DataInst.CVIDBlocks[3] + DataInst.CVIDBlocks[4]\n",
    "DataInst.update_cv_tens(CV_TrIDs, CV_VaIDs, MLOBJLIST)\n",
    "\n",
    "# Determine the best Hyperparameters SIGMA and LAMBDA for this CV step\n",
    "KRR.cv_para(DataInst, MLOBJLIST, optimizer=\"vectorized\", convR=0.001, convM=0.00001, MaxIter=4, ignoreConv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f64f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CV Tensor for training against Hyperparameters.\n",
    "# Here, Block \"3\" for validation.\n",
    "CV_VaIDs = DataInst.CVIDBlocks[3]\n",
    "CV_TrIDs = DataInst.CVIDBlocks[0] + DataInst.CVIDBlocks[1] + DataInst.CVIDBlocks[2] + DataInst.CVIDBlocks[4]\n",
    "DataInst.update_cv_tens(CV_TrIDs, CV_VaIDs, MLOBJLIST)\n",
    "\n",
    "# Determine the best Hyperparameters SIGMA and LAMBDA for this CV step\n",
    "KRR.cv_para(DataInst, MLOBJLIST, optimizer=\"vectorized\", convR=0.001, convM=0.00001, MaxIter=4, ignoreConv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CV Tensor for training against Hyperparameters.\n",
    "# Here, Block \"4\" for validation.\n",
    "CV_VaIDs = DataInst.CVIDBlocks[4]\n",
    "CV_TrIDs = DataInst.CVIDBlocks[0] + DataInst.CVIDBlocks[1] + DataInst.CVIDBlocks[2] + DataInst.CVIDBlocks[3]\n",
    "DataInst.update_cv_tens(CV_TrIDs, CV_VaIDs, MLOBJLIST)\n",
    "\n",
    "# Determine the best Hyperparameters SIGMA and LAMBDA for this CV step\n",
    "KRR.cv_para(DataInst, MLOBJLIST, optimizer=\"vectorized\", convR=0.001, convM=0.00001, MaxIter=4, ignoreConv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905bef1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finalize the Model - take the mean best SIGMA and LAMBDA across all CV steps and train against the full CV set.\n",
    "# Finally try to predict the values of the Test set - and compare how well the trained model does.\n",
    "KRR.final_cv(DataInst, MLOBJLIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a8e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store an overview of the final performance.\n",
    "OID = open(\"FinStats.out\", \"w\") \n",
    "OID.write(\"# Num        Label            PredVal_M        PredVal_R\\n\")\n",
    "for ii in range(len(KRR.FinTestLabelVals)):\n",
    "    OID.write(\"{:5d}       {: 3.7f}       {: 3.7f}       {: 3.7f}\\n\".format(ii, KRR.FinTestLabelVals[ii], KRR.FinPredMVals[ii], KRR.FinPredRVals[ii]))\n",
    "OID.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f64414",
   "metadata": {},
   "source": [
    "# Step 5 - Saving the ML-Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "KRR.save_model(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38807341",
   "metadata": {},
   "source": [
    "# PART B\n",
    "This part can be used to interactively run a test prediction. Note that for actual production, the batchable script \"03\" should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cfca2",
   "metadata": {},
   "source": [
    "# Step 1 - Loading an ML-Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638d81d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load an existing Model and setup the MLOBJLIST as wells as the Descriptor instance.\n",
    "# Note that the MLOBJLIST contains only those Training entries at this point, that were actually used in this exact succession during training the original model.\n",
    "# (Also note, that the succession is in fact in rising order of entries, as the final training will sort them when going through range(0 ... NActive).)\n",
    "MLEntry.get_db_specs()\n",
    "LLabelL   = [\"\"]\n",
    "KRR = MLModel(LLabelL)\n",
    "MLOBJLIST, DescInst = KRR.get_model(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01fc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data to Predict.\n",
    "LType     = \"Prediction\"\n",
    "LQC_Pack  = \"orca\"\n",
    "LQC_low   = \"PM3\"\n",
    "LQC_high  = \"TD-DFT\"\n",
    "LDataL    = [\"Geometry\", \"Mulliken_F\", \"SEmpOrbInfo_F\"]\n",
    "# Read the Prediction IDs and Fingerprints from File. Append to the pre-loaded MLEntries.\n",
    "FID = open (\"\", \"r\")               # add filename here\n",
    "FLoc = FID.readlines()\n",
    "FID.close()\n",
    "LID      = []\n",
    "LFingerP = []\n",
    "PrepList = []\n",
    "for line in range(len(FLoc)):\n",
    "    LID.append(FLoc[line].split()[0])\n",
    "    Aux     = FLoc[line].split()[1]\n",
    "    FingLen = len(Aux.split(\",\"))\n",
    "    LFing = []\n",
    "    for jj in range(FingLen):\n",
    "        LFing.append(int(Aux.split(\",\")[jj]))\n",
    "    LFingerP.append(LFing)\n",
    "    LObj = MLEntry(LID[-1], LType, LFingerP[-1], LQC_Pack, LQC_low, LQC_high, LDataL, LLabelL)\n",
    "    PrepList.append(dc(LObj))\n",
    "# Only needed once per \"newly calculated entry\": Merge Data into my native format\n",
    "for ii in range(len(PrepList)):\n",
    "    PrepList[ii].merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MLEntry.Data from MergeFiles and immediately describe.\n",
    "cnt   = 0\n",
    "tTot  = 0\n",
    "incnt = 0\n",
    "now   = time.time()\n",
    "for ii in range(len(MLOBJLIST)):\n",
    "    MLOBJLIST[ii].get_merged_data()\n",
    "    DescInst.describe(MLOBJLIST[ii])\n",
    "    CurID = MLOBJLIST[ii].ID\n",
    "    print(CurID)\n",
    "    cnt  += 1\n",
    "    Perc  = cnt / len(MLOBJLIST)\n",
    "    UpdThrsh = 0.001\n",
    "    if Perc > UpdThrsh+(incnt*UpdThrsh):\n",
    "        then  = time.time()\n",
    "        tReq  = then-now\n",
    "        tTot += tReq\n",
    "        mReq  = tTot / cnt\n",
    "        Rem   = float(len(MLOBJLIST)-cnt)*mReq\n",
    "        clear_output(wait=True)\n",
    "        STR1  = \"Finished {:.2f} % ({}) of all structures ({}) in {:.1f} seconds. ({})\\n\".format(Perc*100, cnt, len(MLOBJLIST), tTot, timeconv(tTot),)\n",
    "        STR2  = \"Required {:.3f} seconds on average for each structure.\\n\".format(mReq)\n",
    "        STR3  = \"Expecting {:.1f} seconds remaining. ({})\\n\".format(Rem, timeconv(Rem))\n",
    "        print(STR1+STR2+STR3)\n",
    "        incnt += 1\n",
    "        now    = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13302c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DataTens Object to get the precalculated Distance matrices. For predictions, no splitting or stratifying needed, of course.\n",
    "DataInst = DataTens(MLOBJLIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a5ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform predictions from loaded model.\n",
    "KRR.predict_from_loaded(DataInst, MLOBJLIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Predictions to file.\n",
    "FID = open(\"{}.Predictions\".format(LLabelL[0]), \"w\")\n",
    "cnt = 0\n",
    "for ii in range(len(MLOBJLIST)):\n",
    "    if MLOBJLIST[ii].Type == \"Prediction\":\n",
    "        Aux = \"\"\n",
    "        for jj in range(len(MLOBJLIST[ii].FingerP)):\n",
    "            Aux  += \"{},\".format(MLOBJLIST[ii].FingerP[jj])\n",
    "            LFing = Aux.rstrip(Aux[-1])\n",
    "        SSS = \"{}   {}   {: 12.9f}\\n\".format(MLOBJLIST[ii].ID, LFing, KRR.Predictions[cnt])\n",
    "        FID.write(SSS)\n",
    "        cnt += 1\n",
    "FID.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
